# Архитектура Системы

## Высокоуровневая схема

```mermaid
graph TD
    User[Пользователь] -->|Взаимодействие| UI[Streamlit Frontend]
    UI -->|HTTP Запросы| API[FastAPI Backend]
    
    subgraph Backend Services
        API -->|Загрузка/Удаление| Storage[Supabase Client]
        API -->|Чат/Поиск| LLM[LLM Service]
        API -->|Индексация| Ingestion[Ingestion Service]
    end
    
    Ingestion -->|1. Извлечение текста| Parser[Text Parser (pypdf)]
    Ingestion -->|2. Разделение на чанки| Chunker[Chunker]
    Ingestion -->|3. Векторизация| Embedder[OpenAI Embeddings]
    Ingestion -->|4. Сохранение| VectorDB[(Supabase PostgreSQL)]
    
    LLM -->|Векторизация запроса| Embedder
    LLM -->|Поиск (RPC)| VectorDB
    LLM -->|Генерация ответа| OpenAI[OpenAI GPT]
```

## Компоненты

### 1. Frontend (Streamlit)
Клиентская часть приложения, написанная на Python с использованием библиотеки Streamlit.
*   **Файл**: `frontend/app.py`
*   **Функции**:
    *   Отображение формы загрузки файлов.
    *   Отображение списка документов и их статусов.
    *   Интерфейс чата (история сообщений, поле ввода).
    *   Взаимодействие с Backend через REST API (requests).

### 2. Backend (FastAPI)
Серверная часть, обеспечивающая бизнес-логику.
*   **Файл**: `backend/main.py`
*   **Endpoints**:
    *   `POST /documents/upload`: Принимает файл, создает запись в БД и запускает фоновую задачу индексации.
    *   `POST /chat`: Принимает вопрос, выполняет поиск и возвращает ответ LLM.
*   **Асинхронность**: Индексация документов выполняется в фоне (`BackgroundTasks`), чтобы не блокировать интерфейс пользователя.

### 3. Сервисы (Backend Services)

#### Ingestion Service (`backend/services/ingestion.py`)
Отвечает за обработку загруженных файлов.
1.  **Extract**: Извлекает "сырой" текст из PDF или TXT.
2.  **Chunk**: Разбивает текст на кусочки (например, по 1000 символов с перекрытием 200). Это нужно, чтобы текст поместился в контекстное окно LLM и поиск был более точным.
3.  **Embed**: Отправляет каждый чанк в OpenAI API для получения векторного представления (список из 1536 чисел).
4.  **Store**: Сохраняет текст и вектор в таблицу `chunks`.

#### LLM Service (`backend/services/llm.py`)
Отвечает за интеллектуальную часть.
1.  **get_embedding**: Получает вектор для поискового запроса.
2.  **generate_answer**: Формирует системный промпт ("Отвечай только на основе контекста..."), добавляет найденные чанки и отправляет запрос в GPT-4o.

### 4. База Данных (Supabase + pgvector)
Хранилище данных и движок поиска.
*   **Таблица `sources`**: Метаданные файлов (имя, статус, дата).
*   **Таблица `chunks`**: Фрагменты текста и поле типа `vector(1536)`.
*   **Функция `match_chunks`**: Хранимая процедура PL/pgSQL. Выполняет косинусное сходство (`cosine similarity`) между вектором запроса и векторами в базе, возвращая наиболее релевантные куски текста.

## Поток Данных (RAG Workflow)

### Процесс Индексации (Write Path)
1.  Пользователь загружает файл.
2.  Создается запись `source` со статусом `uploaded`.
3.  Фоновый процесс парсит файл и разбивает на чанки.
4.  Для чанков генерируются эмбеддинги.
5.  Чанки сохраняются в БД.
6.  Статус обновляется на `indexed`.

### Процесс Поиска и Ответа (Read Path)
1.  Пользователь задает вопрос: *"Как настроить проект?"*
2.  Backend превращает вопрос в вектор `$query_vec`.
3.  Backend вызывает `rpc('match_chunks', query_embedding=$query_vec)`.
4.  БД возвращает Топ-5 самых похожих чанков.
5.  Backend формирует промпт:
    ```text
    Context:
    - [Chunk 1 text...]
    - [Chunk 2 text...]

    Question: Как настроить проект?
    ```
6.  LLM генерирует ответ на основе переданного контекста.
7.  Ответ и источники возвращаются на Frontend.
