# Архитектура Системы

## Высокоуровневая схема

```mermaid
graph TD
    User[Пользователь] -->|Взаимодействие| UI[Streamlit Frontend]
    UI -->|HTTP Запросы| API[FastAPI Backend]
    
    subgraph Backend Services
        API -->|Загрузка/Удаление| Storage[Supabase Client]
        API -->|Чат/Поиск| LLM[LLM Service]
        API -->|Индексация| Ingestion[Ingestion Service]
    end
    
    Ingestion -->|1. Извлечение текста| Parser[Text Parser (pypdf)]
    Ingestion -->|2. Разделение на чанки| Chunker[Chunker]
    Ingestion -->|3. Векторизация| Embedder[OpenAI Embeddings]
    Ingestion -->|4. Сохранение| VectorDB[(Supabase PostgreSQL)]
    
    LLM -->|Векторизация запроса| Embedder
    LLM -->|Поиск (RPC)| VectorDB
    LLM -->|Reranking| Reranker[LLM Reranker]
    LLM -->|Caching| Cache[In-memory Cache]
    LLM -->|Генерация ответа| OpenAI[OpenAI GPT]
```

## Компоненты

### 1. Frontend (Streamlit)
Клиентская часть приложения, написанная на Python с использованием библиотеки Streamlit.
*   **Файл**: `frontend/app.py`
*   **Функции**:
    *   Отображение формы загрузки файлов.
    *   Отображение списка документов и их статусов.
    *   Интерфейс чата (история сообщений, поле ввода).
    *   Взаимодействие с Backend через REST API (requests).

### 2. Backend (FastAPI)
Серверная часть, обеспечивающая бизнес-логику.
*   **Файл**: `backend/main.py`
*   **Endpoints**:
    *   `POST /documents/upload`: Принимает файл, создает запись в БД и запускает фоновую задачу индексации.
    *   `POST /chat`: Принимает вопрос, выполняет поиск и возвращает ответ LLM.
*   **Асинхронность**: Индексация документов выполняется в фоне (`BackgroundTasks`), чтобы не блокировать интерфейс пользователя.

### 3. Сервисы (Backend Services)

#### Ingestion Service (`backend/services/ingestion.py`)
Отвечает за обработку загруженных файлов.
1.  **Extract**: Извлекает "сырой" текст из PDF или TXT.
2.  **Chunk**: Разбивает текст на кусочки (например, по 1000 символов с перекрытием 200). Это нужно, чтобы текст поместился в контекстное окно LLM и поиск был более точным.
3.  **Embed**: Отправляет каждый чанк в OpenAI API для получения векторного представления (список из 1536 чисел).
4.  **Store**: Сохраняет текст и вектор в таблицу `chunks`.

#### LLM Service (`backend/services/llm.py`)
Отвечает за интеллектуальную часть.
1.  **get_embedding**: Получает вектор для поискового запроса.
2.  **generate_answer**: Формирует системный промпт ("Отвечай только на основе контекста..."), добавляет найденные чанки и отправляет запрос в GPT-4o.

### 4. База Данных (Supabase + pgvector)
Хранилище данных и движок поиска.
*   **Таблица `sources`**: Метаданные файлов (имя, статус, дата).
*   **Таблица `chunks`**: Фрагменты текста и поле типа `vector(1536)`.
*   **Таблица `chunks`**: Фрагменты текста и поле типа `vector(1536)`.
*   **Гибридный Поиск (Hybrid Search)**:
    *   **Семантический**: `rpc('match_chunks')` через `pgvector`.
    *   **По ключевым словам**: `rpc('match_chunks_keyword')` через Postgres Full-Text Search (FTS).
    *   **RRF Fusion**: Результаты объединяются алгоритмом Reciprocal Rank Fusion для улучшения качества выдачи.
    *   **Reranking**: (Project 11) Повторное ранжирование Топ-K кандидатов с помощью LLM для повышения релевантности (Cross-Encoder / LLM Scoring approach).

### 5. Caching Layer
*   **Тип**: In-memory (LRU + TTL).
*   **Ключ**: MD5 hash от нормализованного вопроса и параметров конфигурации.
*   **Цель**: Мгновенный ответ на повторяющиеся вопросы без обращения к БД и LLM.

## Поток Данных (RAG Workflow)

### Процесс Индексации (Write Path)
1.  Пользователь загружает файл.
2.  Создается запись `source` со статусом `uploaded`.
3.  Фоновый процесс парсит файл и разбивает на чанки.
4.  Для чанков генерируются эмбеддинги.
5.  Чанки сохраняются в БД.
6.  Статус обновляется на `indexed`.

### Процесс Поиска и Ответа (Read Path)
1.  Пользователь задает вопрос: *"Как настроить проект?"*
1.  Пользователь задает вопрос: *"Как настроить проект?"*
2.  Backend запускает параллельно:
    *   **Semantic Search**: Векторизует вопрос и ищет через `pgvector`.
    *   **Keyword Search**: Ищет точные совпадения через FTS.
    *   **Keyword Search**: Ищет точные совпадения через FTS.
3.  Backend объединяет результаты (RRF Fusion).
4.  **Reranking**: Backend отправляет top-N кандидатов в LLM для переоценки релевантности (если включено).
5.  Backend берет финальный топ лучших (например, 5).
4.  Backend формирует промпт:
    ```text
    Context:
    - [Chunk 1 text...]
    - [Chunk 2 text...]

    Question: Как настроить проект?
    ```
5.  LLM генерирует ответ на основе переданного контекста.
6.  Ответ и источники (с RRF score) возвращаются на Frontend.
